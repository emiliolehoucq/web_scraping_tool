# Functions to scrape websites
# Emilio Lehoucq
# This script uses code generated by ChatGPT and GitHub Copilot

# Importing libraries
from requests import get
from requests import exceptions
from selenium import webdriver
from url_modifiers import modify_url_school_jobs
from ssl_connection_settings import get_legacy_session
from bs4 import BeautifulSoup
from time import sleep

# Setting user agent

my_user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# Function using requests

def get_response(url, verbose=False):
    '''
    Function to make requests.
    '''
    # I don't want an error in a single request to derail the whole script
    try:
        # If it's a schooljobs.com URL, use legacy session
        if 'schooljobs.com' in url:
            url = modify_url_school_jobs(url)
            return get_legacy_session().get(url)
        
        # Set user agent
        # https://www.zenrows.com/blog/python-requests-user-agent#set-user-agent
        headers = {'User-Agent': my_user_agent}
        
        # Otherwise, make a standard request to the website
        response = get(url, verify = False, headers = headers) # ignore SSL errors https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification

        # Dealing with iframes for ICMS
        if 'icims.com' in url:
            # Parse the HTML content of the website
            soup = BeautifulSoup(response.text, 'html.parser')
            # Find iframe with actual website
            iframes = soup.find_all('iframe')
            # Iterate over each iframe
            for iframe in iframes:
                # Check if there's a URL from icims
                iframe_url = iframe['src']
                if '.icims.com' in iframe_url:
                    response = get(iframe_url, verify = False, headers = headers) # ignore SSL errors

        # Return response
        return response
    
    # If it doesn't work, print some information and return None
    except exceptions.RequestException as e:
        # if verbose:
        #     print('\n----------------------------------------')
        #     print('Error in get_response function!')
        #     print(f'Failed to load page: \n\n{e}\n')
        #     print(f'URL that failed: \n\n{url}\n')
        #     print(f"Traceback: \n\n{traceback.format_exc()}\n")
        #     print('----------------------------------------\n')
        return None

# Function using selenium

def get_selenium_response(url, verbose=False, headless=True):
    '''
    Function to scrape a website using Selenium.
    '''
    try:
        # Setting options https://www.selenium.dev/documentation/webdriver/drivers/options/
        options = webdriver.ChromeOptions()
        # Ask browser to ignore SSL errors
        # I think equivalent to options.add_argument('ignore-certificate-errors')
        options.accept_insecure_certs = True
        # Add user agent https://www.zenrows.com/blog/user-agent-web-scraping#best
        options.add_argument(f"--user-agent={my_user_agent}")
        # Add headless option
        if headless:
            options.add_argument('--headless')

        # Create driver
        driver = webdriver.Chrome(options = options)

        # Get URL
        driver.get(url)

        # Dealing with iframes for ICMS
        if 'icims.com' in url:
            # Switch to iframe
            # https://www.selenium.dev/documentation/webdriver/interactions/frames/
            # This seems to behave differently if operating in headless mode or not!!!
            driver.switch_to.frame('icims_content_iframe')

        # # Dealing with AngularJS for Interfolio
        # # This worked for https://apply.interfolio.com/136320
        # if 'interfolio.com' in url:
        #     # This may not always work and there are more sophisticated solutions: https://www.selenium.dev/documentation/webdriver/waits/
        #     sleep(10)

        # Since sleep() "magically" worked for Interfolio, I'll add some wait for every case
        # I don't care much about speed, so might as well...
        sleep(10)

        # Get response
        response = driver.page_source

        # Close driver
        driver.quit()

        # Return response
        return response

    # If it doesn't work, print some information and return None
    except Exception as e:
        # if verbose:
        #     print('\n----------------------------------------')
        #     print('Error in get_selenium_response function!')
        #     print(f'Failed to load page: \n\n{e}\n')
        #     print(f'URL that failed: \n\n{url}\n')
        #     print(f"Traceback: \n\n{traceback.format_exc()}\n")
        #     print('----------------------------------------\n')
        return None

if __name__ == '__main__':
    print('Module with functions to scrape websites run successfully!')